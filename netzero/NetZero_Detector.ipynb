{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d3f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import streamlit as st\n",
    "from streamlit_jupyter import StreamlitPatcher\n",
    "StreamlitPatcher().jupyter()\n",
    "from nbdev.export import nb_export\n",
    "\n",
    "# Set page configuration\n",
    "st.set_page_config(layout=\"wide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29965ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# generic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "# pdf processing functions\n",
    "import fitz\n",
    "import pdf_utils\n",
    "\n",
    "# NLP models\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7193dd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ambar\\anaconda3\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "# create pipeline for environmental classifier\n",
    "name = \"ESGBERT/EnvironmentalBERT-environmental\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, max_len=512)\n",
    "esg_pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "810a83ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "# create pipeline for netzero-reduction classifier\n",
    "name = \"ClimatePolicyRadar/national-climate-targets\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, max_len=512)\n",
    "cpr_pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, function_to_apply=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a80d52fe-f9dd-4d0e-a3cd-4b663e54e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# function to encapsulate all preprocessing steps\n",
    "def pdf_to_sentences(path, max_char, min_words=5):\n",
    "    text = pdf_utils.true_pdf2text(path=path) # check if true PDF\n",
    "    \n",
    "    if len(text) == 0:\n",
    "        true_pdf = False\n",
    "        \n",
    "        # OCR processing time based on no. of pages\n",
    "        doc = fitz.open(stream=path, filetype=\"pdf\")\n",
    "        est_time = round(5.0*len(doc)/60) \n",
    "        l = [\"‚è≥Running preprocessing... Estimated time remaining:\", str(est_time), \"mins\"]\n",
    "        placeholder = app.empty()\n",
    "        placeholder.write(\" \".join(l))\n",
    "\n",
    "        # run OCR extraction\n",
    "        output_dir = r'D:\\Work\\Climate NLP\\web apps\\pdf_images'\n",
    "        text = pdf_utils.scan_pdf2text(path=path, output_dir=output_dir)\n",
    "        \n",
    "        placeholder.empty()\n",
    "    \n",
    "    text = text[:max_char] # spacy has a 1M char limit\n",
    "    sents = pdf_utils.text2sents(text=text, min_words=min_words)\n",
    "    \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd036000-f38c-495f-9837-dd3d4a6269ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# function to run a 2-stage classification\n",
    "def classify(path, cutoff_score, max_char=1000000):\n",
    "    # run preprocessing steps\n",
    "    sents = pdf_to_sentences(path, max_char)\n",
    "    \n",
    "    # NLP processing time per sentence based on test runs\n",
    "    est_time = round(0.1*len(sents)/60) \n",
    "    l = [\"‚è≥Running classification... Estimated time remaining:\", str(est_time), \"mins\"]\n",
    "    placeholder = app.empty()\n",
    "    placeholder.write(\" \".join(l))\n",
    "\n",
    "    # run stage-1 classifier\n",
    "    s1_class = esg_pipe(sents, padding=True, truncation=True)\n",
    "    \n",
    "    # create data frame with texts labeled as environmental\n",
    "    env_labels = [x['label'] for x in s1_class]\n",
    "    df = pd.DataFrame({\"text\": sents, \"env_label\": env_labels})\n",
    "    df = df[df['env_label'] == \"environmental\"]\n",
    "    \n",
    "    # print count of total vs env sentences\n",
    "    app.subheader(\"Document Summary\", divider=\"blue\")\n",
    "    l = [str(len(sents)), \"sentences in document.\", str(len(df)), \"classified as Environmental.\"]\n",
    "    app.write(\" \".join(l))\n",
    "    \n",
    "    # run stage-2 classifier\n",
    "    sents_s1 = df.text.to_list()\n",
    "    s2_class = cpr_pipe(sents_s1, padding=True, truncation=True)\n",
    "    \n",
    "    # merge stage-2 labels into df\n",
    "    df['label'] = [x['label'] for x in s2_class]\n",
    "    df['score'] = [x['score'] for x in s2_class]\n",
    "    \n",
    "    # set labels to None where score is below cutoff\n",
    "    df.loc[df['score'] < cutoff_score, 'label'] = 'None'\n",
    "    \n",
    "    placeholder.empty()\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27885ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# generate content for About page\n",
    "def setup_about_page(file, img1, img2):\n",
    "    # demo data download\n",
    "    with open(file, \"rb\") as pdf_file:\n",
    "        demo_data = pdf_file.read()\n",
    "    \n",
    "    txt_demo = '''üëãWelcome to Scetti's Netzero-Reduction Detector tool!\n",
    "    If you do not have a policy document at hand, you can try out the app with a sample document.'''\n",
    "    ack.markdown(txt_demo)\n",
    "    ack.download_button(label=\"Download demo document\",\n",
    "                        data=demo_data,\n",
    "                        file_name=\"demo_policy_document.pdf\",\n",
    "                        mime='application/octet-stream')    \n",
    "    \n",
    "    # methodology notes\n",
    "    ack.subheader(\"Methodology\", divider=\"blue\")\n",
    "    ack.markdown(\"Each document is analyzed through a 3-stage process described below\")\n",
    "    ack.image(img1)\n",
    "\n",
    "    # preprocessing flowchart\n",
    "    ack.markdown(\":one: The extraction of sentences from a PDF document goes through a series of steps\")\n",
    "    ack.image(img2)\n",
    "    \n",
    "    # acknowledgements\n",
    "    ack_file = r'D:\\Work\\Climate NLP\\web apps\\acknowledgements.txt'\n",
    "    with open(ack_file, \"r\") as file:\n",
    "        ack_txt = file.readlines()\n",
    "    ack.markdown(ack_txt[1]) # ESGBERT/environmental acknowledgement\n",
    "    ack.markdown(ack_txt[3]) # ClimatePolicyRadar acknowledgement\n",
    "    \n",
    "    # contact info\n",
    "    txt_fb = '''üìßTell us about your experience with the Netzero-Reduction Detector! Please send your \n",
    "    feedback and suggestions to <a href='mailto:ambar@scetti.org'> ambar@scetti.org</a>'''\n",
    "    ack.markdown(txt_fb, unsafe_allow_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1120d7cd-e287-42f1-908e-2f719f1ad699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# function to keep count of app runs\n",
    "def update_run_count():\n",
    "    run_count_file = r'D:\\Work\\Climate NLP\\web apps\\run_count.txt'\n",
    "\n",
    "    # read current value of run_count from file\n",
    "    with open(run_count_file, \"r\") as file:\n",
    "        run_count = int(file.read())\n",
    "\n",
    "    # increment the count and write it back to the file\n",
    "    run_count += 1\n",
    "    with open(run_count_file, \"w\") as file:\n",
    "        file.write(str(run_count))\n",
    "\n",
    "    # st.write(f\"This Streamlit app has been run {run_count}¬†times.\")\n",
    "    return run_count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b910b260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Netzero-Reduction Detector"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 12:31:50.916 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\ambar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef95121264dc4280994080d6a0666ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Highlight target text within document', indent=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|export\n",
    "st.title(\"Netzero-Reduction Detector\")\n",
    "\n",
    "# user input widgets (form inside sidebar)\n",
    "with st.sidebar:\n",
    "    with st.form(\"user_inputs\"):\n",
    "        uploaded_file = st.file_uploader(label=\"Upload a PDF document\",\n",
    "                                         type=[\".pdf\"])\n",
    "        \n",
    "        annotate_pdf = st.checkbox(\"Highlight target text within document\", \n",
    "                                   value=True,\n",
    "                                   help=\"Not supported for scanned PDFs\")\n",
    "        \n",
    "        cutoff_score = st.slider(label=\"Minimum confidence threshold (recommended 0.50)\",\n",
    "                                 help=\"Setting this too low can produce false positives\",\n",
    "                                 min_value=0.0, \n",
    "                                 max_value=1.0, \n",
    "                                 value=0.5, \n",
    "                                 step=0.05)\n",
    "        \n",
    "        submitted = st.form_submit_button(\"Analyze document\")\n",
    "\n",
    "# create separate tabs for analysis and acknowledgements\n",
    "app, ack = st.tabs([\"üìàAnalysis\", \"üì¢About\"])\n",
    "\n",
    "# setup About page\n",
    "x = setup_about_page(file = r'D:\\Work\\Climate NLP\\web apps\\demo_policy_document.pdf',\n",
    "                     img1 = r'D:\\Work\\Climate NLP\\web apps\\nlp_process_flow.png',\n",
    "                     img2 = r'D:\\Work\\Climate NLP\\web apps\\preprocessing_flowchart.png')\n",
    "\n",
    "# description for Analysis tab\n",
    "txt_intro = '''As a researcher working on climate policy, have you tried searching for measurable goals or targets relating to GHG emission reduction? \n",
    "This app uses AI-based Natural Language Processing to detect occurences of Netzero, Reduction, and Other targets in your climate policy document.\n",
    "\n",
    "üåü:green[June 2024 update: scanned/image PDF documents are now supported!]'''\n",
    "app.markdown(txt_intro)\n",
    "\n",
    "if uploaded_file and submitted:\n",
    "    start_time = time.time()\n",
    "    x = update_run_count()\n",
    "    true_pdf = True # boolean flag for true vs scanned PDF\n",
    "    \n",
    "    # run 2-stage classification on a PDF (set max_char=1000 for testing)\n",
    "    path = uploaded_file.read()\n",
    "    df = classify(path=path, cutoff_score=cutoff_score)\n",
    "    \n",
    "    if df is not None:\n",
    "        # plot stage-2 label freq\n",
    "        chart_data = df.groupby('label').size().reset_index(name='sentences')\n",
    "        app.bar_chart(chart_data, \n",
    "                     x='label', \n",
    "                     y='sentences')\n",
    "\n",
    "        # print sentences not classified as 'None'\n",
    "        app.subheader(\"Text classified as Netzero, Reduction, or Other target\", divider=\"blue\")\n",
    "        app.markdown(\":information_source: Double-click on text to expand. Hover on top right corner of table for more options.\")\n",
    "        df1 = df[df.label!='None'].drop('env_label', axis=1)\n",
    "        app.dataframe(df1, \n",
    "                     hide_index=True, \n",
    "                     use_container_width=True)\n",
    "\n",
    "        # display PDF with target text highlighted (not supported for scanned PDF)\n",
    "        if annotate_pdf and true_pdf:\n",
    "            target_text = df1.text.to_list()\n",
    "            pages = pdf_utils.annotate_target_text(path=path, target_text=target_text)\n",
    "            for page in pages:\n",
    "                app.image(page, use_column_width=True)\n",
    "\n",
    "        # print completion status\n",
    "        app.write(\"Analysis completed! üëç\")\n",
    "        time_taken = round((time.time() - start_time)/60, 1)\n",
    "        l = [\"Time taken:\", str(time_taken), \"mins\"]\n",
    "        app.write(\" \".join(l))\n",
    "\n",
    "        # CSV file for user download\n",
    "        file_name = uploaded_file.name.replace('pdf', 'csv')\n",
    "        csv = df.to_csv(index=False).encode('utf-8')\n",
    "        app.download_button(label=\"Download CSV output\", \n",
    "                            data=csv, \n",
    "                            file_name=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18128341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports this notebook to a .py file with streamlit statements added\n",
    "nb_export(\"NetZero_Detector.ipynb\", lib_path=\"./\", name=\"NetZero_Detector\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
