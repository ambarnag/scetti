{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d3f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import streamlit as st\n",
    "from streamlit_jupyter import StreamlitPatcher\n",
    "StreamlitPatcher().jupyter()\n",
    "from nbdev.export import nb_export\n",
    "\n",
    "# Set page configuration\n",
    "st.set_page_config(layout=\"wide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29965ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# generic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "# file and os related libs\n",
    "import os\n",
    "import platform\n",
    "import glob\n",
    "\n",
    "import fitz # extract text from true PDF\n",
    "import pytesseract # extract text from scanned PDF\n",
    "import spacy # parsing text into sentences\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline # NLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7193dd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ambar\\anaconda3\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "# create pipeline for environmental classifier\n",
    "name = \"ESGBERT/EnvironmentalBERT-environmental\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, max_len=512)\n",
    "esg_pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "810a83ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "# create pipeline for netzero-reduction classifier\n",
    "name = \"ClimatePolicyRadar/national-climate-targets\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, max_len=512)\n",
    "cpr_pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, function_to_apply=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099266d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# function to extract text from a 'true' PDF\n",
    "def true_pdf2text(path):\n",
    "    # doc = fitz.open(path) # uncomment to test on local file\n",
    "    doc = fitz.open(stream=path, filetype=\"pdf\")\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d152a186-5ec4-4fab-95e6-9ddecc8bcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# get tesseract installation path based on operating system\n",
    "def get_tesseract_path():\n",
    "    current_os = platform.system() # detect OS on which python is running\n",
    "    \n",
    "    if current_os == 'Windows':\n",
    "        res = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "    elif current_os == 'Darwin':  # macOS\n",
    "        res = '/usr/local/bin/tesseract'\n",
    "    elif current_os == 'Linux':\n",
    "        res = '/usr/bin/tesseract' # default path on most Linux distributions\n",
    "    else:\n",
    "        raise Exception(f'Unsupported operating system: {current_os}')\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e588f087-4d02-436a-94e9-66b446891351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# function to extract text from a 'scanned' PDF using OCR\n",
    "def scan_pdf2text(path):\n",
    "    pytesseract.pytesseract.tesseract_cmd = get_tesseract_path()\n",
    "    \n",
    "    # doc = fitz.open(path) # uncomment to test on local file\n",
    "    doc = fitz.open(stream=path, filetype=\"pdf\")\n",
    "    \n",
    "    # OCR processing time based on no. of pages\n",
    "    est_time = round(5.0*len(doc)/60) \n",
    "    l = [\"‚è≥Running preprocessing... Estimated time remaining:\", str(est_time), \"mins\"]\n",
    "    placeholder = app.empty()\n",
    "    placeholder.write(\" \".join(l))\n",
    "    \n",
    "    # create a temp dir to store .png files\n",
    "    output_dir = r'D:\\Work\\Climate NLP\\web apps\\pdf_images'\n",
    "    unique_id = str(uuid.uuid4())[:8]\n",
    "    newdir = os.path.join(output_dir, unique_id)\n",
    "    os.mkdir(newdir)\n",
    "    \n",
    "    # convert pages to images > run OCR on image files\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        img = page.get_pixmap(dpi=300)  # 300 dpi is high resolution\n",
    "        img_file = os.path.join(newdir, f\"page-{page.number}.png\")\n",
    "        img.save(img_file)\n",
    "        text += pytesseract.image_to_string(img_file)\n",
    "        \n",
    "    # delete image files and temp dir\n",
    "    files = glob.glob(os.path.join(newdir, \"*.png\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    os.rmdir(newdir)\n",
    "    \n",
    "    placeholder.empty()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6340b778-b5d9-4a93-a3c5-815cc622566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# function to extract sentences from text using spacy\n",
    "def text2sents(text):\n",
    "    # returns no. of words in a sentence\n",
    "    def no_words(txt):\n",
    "        return len(txt.strip().split())\n",
    "    \n",
    "    # parse text into sentences using spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    text = nlp(text)\n",
    "    sentences = list(map(str, text.sents))\n",
    "    \n",
    "    # clean up sentences\n",
    "    sentences = [x.replace(\"\\n\", \"\") for x in sentences] # remove newline chars\n",
    "    sentences = [x for x in sentences if x != \"\"] # remove blanks\n",
    "    sentences = [x for x in sentences if x[0].isupper()] # sentence should start with uppercase\n",
    "    sentences = [x for x in sentences if not \".....\" in x] # remove table of contents\n",
    "    sentences = [x for x in sentences if no_words(x) > 4] # remove sentences shorter than 5 words\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7c2a8e5-517b-4884-8da2-b7c77014a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# function to encapsulate all preprocessing steps\n",
    "def pdf_to_sentences(path, max_char):\n",
    "    text = true_pdf2text(path=path) # check if true PDF\n",
    "    \n",
    "    if len(text) == 0:\n",
    "        true_pdf = False\n",
    "        text = scan_pdf2text(path=path)\n",
    "    \n",
    "    text = text[:max_char] # spacy has a 1M char limit\n",
    "    sents = text2sents(text=text)\n",
    "    \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd036000-f38c-495f-9837-dd3d4a6269ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# function to run a 2-stage classification\n",
    "def classify(path, cutoff_score, max_char=1000000):\n",
    "    # run preprocessing steps\n",
    "    sents = pdf_to_sentences(path, max_char)\n",
    "    \n",
    "    # NLP processing time per sentence based on test runs\n",
    "    est_time = round(0.1*len(sents)/60) \n",
    "    l = [\"‚è≥Running classification... Estimated time remaining:\", str(est_time), \"mins\"]\n",
    "    placeholder = app.empty()\n",
    "    placeholder.write(\" \".join(l))\n",
    "\n",
    "    # run stage-1 classifier\n",
    "    s1_class = esg_pipe(sents, padding=True, truncation=True)\n",
    "    \n",
    "    # create data frame with texts labeled as environmental\n",
    "    env_labels = [x['label'] for x in s1_class]\n",
    "    df = pd.DataFrame({\"text\": sents, \"env_label\": env_labels})\n",
    "    df = df[df['env_label'] == \"environmental\"]\n",
    "    \n",
    "    # print count of total vs env sentences\n",
    "    app.subheader(\"Document Summary\", divider=\"blue\")\n",
    "    l = [str(len(sents)), \"sentences in document.\", str(len(df)), \"classified as Environmental.\"]\n",
    "    app.write(\" \".join(l))\n",
    "    \n",
    "    # run stage-2 classifier\n",
    "    sents_s1 = df.text.to_list()\n",
    "    s2_class = cpr_pipe(sents_s1, padding=True, truncation=True)\n",
    "    \n",
    "    # merge stage-2 labels into df\n",
    "    df['label'] = [x['label'] for x in s2_class]\n",
    "    df['score'] = [x['score'] for x in s2_class]\n",
    "    \n",
    "    # set labels to None where score is below cutoff\n",
    "    df.loc[df['score'] < cutoff_score, 'label'] = 'None'\n",
    "    \n",
    "    placeholder.empty()\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5226301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# function to annotate PDF for all target text found\n",
    "def annotate_target_text(path, target_text):    \n",
    "    with fitz.open(stream=path, filetype=\"pdf\") as doc:\n",
    "        for page in doc:\n",
    "            found = False\n",
    "            for text in target_text:\n",
    "                areas = page.search_for(text)\n",
    "                if len(areas) > 0:\n",
    "                    found = True\n",
    "                    for area in areas:\n",
    "                        page.add_rect_annot(area) # mark red box around target text(s)\n",
    "            if found:\n",
    "                # display page only if it contains a target\n",
    "                pix = page.get_pixmap(dpi=200).tobytes()\n",
    "                app.image(pix, use_column_width=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27885ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# generate content for About page\n",
    "def setup_about_page(file, img1, img2):\n",
    "    # demo data download\n",
    "    with open(file, \"rb\") as pdf_file:\n",
    "        demo_data = pdf_file.read()\n",
    "    \n",
    "    txt_demo = '''üëãWelcome to Scetti's Netzero-Reduction Detector tool!\n",
    "    If you do not have a policy document at hand, you can try out the app with a sample document.'''\n",
    "    ack.markdown(txt_demo)\n",
    "    ack.download_button(label=\"Download demo document\",\n",
    "                        data=demo_data,\n",
    "                        file_name=\"demo_policy_document.pdf\",\n",
    "                        mime='application/octet-stream')    \n",
    "    \n",
    "    # methodology notes\n",
    "    ack.subheader(\"Methodology\", divider=\"blue\")\n",
    "    ack.markdown(\"Each document is analyzed through a 3-stage process described below\")\n",
    "    ack.image(img1)\n",
    "    \n",
    "    # acknowledgements\n",
    "    ack_file = r'D:\\Work\\Climate NLP\\web apps\\acknowledgements.txt'\n",
    "    with open(ack_file, \"r\") as file:\n",
    "        ack_txt = file.readlines()\n",
    "    ack.markdown(ack_txt[1]) # ESGBERT/environmental acknowledgement\n",
    "    ack.markdown(ack_txt[3]) # ClimatePolicyRadar acknowledgement\n",
    "    \n",
    "    # preprocessing flowchart\n",
    "    ack.markdown(\"The extraction of sentences from a PDF document goes through a series of steps\")\n",
    "    ack.image(img2)\n",
    "    \n",
    "    # contact info\n",
    "    txt_fb = '''üìßTell us about your experience with the Netzero-Reduction Detector! Please send your \n",
    "    feedback and suggestions to <a href='mailto:ambar@scetti.org'> ambar@scetti.org</a>'''\n",
    "    ack.markdown(txt_fb, unsafe_allow_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1120d7cd-e287-42f1-908e-2f719f1ad699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# function to keep count of app runs\n",
    "def update_run_count():\n",
    "    run_count_file = r'D:\\Work\\Climate NLP\\web apps\\run_count.txt'\n",
    "\n",
    "    # read current value of run_count from file\n",
    "    with open(run_count_file, \"r\") as file:\n",
    "        run_count = int(file.read())\n",
    "\n",
    "    # increment the count and write it back to the file\n",
    "    run_count += 1\n",
    "    with open(run_count_file, \"w\") as file:\n",
    "        file.write(str(run_count))\n",
    "\n",
    "    # st.write(f\"This Streamlit app has been run {run_count}¬†times.\")\n",
    "    return run_count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b910b260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Netzero-Reduction Detector"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 17:56:41.408 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\ambar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511b006d302e4737a26f7713a8bb9c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Highlight target text within document', indent=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|export\n",
    "st.title(\"Netzero-Reduction Detector\")\n",
    "\n",
    "# user input widgets (form inside sidebar)\n",
    "with st.sidebar:\n",
    "    with st.form(\"user_inputs\"):\n",
    "        uploaded_file = st.file_uploader(label=\"Upload a PDF document\",\n",
    "                                         type=[\".pdf\"])\n",
    "        \n",
    "        annotate_pdf = st.checkbox(\"Highlight target text within document\", \n",
    "                                   value=True,\n",
    "                                   help=\"Not supported for scanned PDFs\")\n",
    "        \n",
    "        cutoff_score = st.slider(label=\"Minimum confidence threshold (recommended 0.50)\",\n",
    "                                 help=\"Setting this too low can produce false positives\",\n",
    "                                 min_value=0.0, \n",
    "                                 max_value=1.0, \n",
    "                                 value=0.5, \n",
    "                                 step=0.05)\n",
    "        \n",
    "        submitted = st.form_submit_button(\"Analyze document\")\n",
    "\n",
    "# create separate tabs for analysis and acknowledgements\n",
    "app, ack = st.tabs([\"üìàAnalysis\", \"üì¢About\"])\n",
    "\n",
    "# setup About page\n",
    "x = setup_about_page(file = r'D:\\Work\\Climate NLP\\web apps\\demo_policy_document.pdf',\n",
    "                     img1 = r'D:\\Work\\Climate NLP\\web apps\\nlp_process_flow.png',\n",
    "                     img2 = r'D:\\Work\\Climate NLP\\web apps\\preprocessing_flowchart.png')\n",
    "\n",
    "# description for Analysis tab\n",
    "txt_intro = '''As a researcher working on climate policy, have you tried searching for measurable goals or targets relating to GHG emission reduction? \n",
    "This app uses AI-based Natural Language Processing to detect occurences of Netzero, Reduction, and Other targets in your climate policy document.\n",
    "\n",
    "üåü:green[June 2024 update: scanned/image PDF documents are now supported!]'''\n",
    "app.markdown(txt_intro)\n",
    "\n",
    "if uploaded_file and submitted:\n",
    "    start_time = time.time()\n",
    "    x = update_run_count()\n",
    "    true_pdf = True # boolean flag for true vs scanned PDF\n",
    "    \n",
    "    # run 2-stage classification on a PDF (set max_char=1000 for testing)\n",
    "    path = uploaded_file.read()\n",
    "    df = classify(path=path, cutoff_score=cutoff_score)\n",
    "    \n",
    "    if df is not None:\n",
    "        # plot stage-2 label freq\n",
    "        chart_data = df.groupby('label').size().reset_index(name='sentences')\n",
    "        app.bar_chart(chart_data, \n",
    "                     x='label', \n",
    "                     y='sentences')\n",
    "\n",
    "        # print sentences not classified as 'None'\n",
    "        app.subheader(\"Text classified as Netzero, Reduction, or Other target\", divider=\"blue\")\n",
    "        df1 = df[df.label!='None'].drop('env_label', axis=1)\n",
    "        app.dataframe(df1, \n",
    "                     hide_index=True, \n",
    "                     use_container_width=True)\n",
    "\n",
    "        # display PDF with target text highlighted (not supported for scanned PDF)\n",
    "        if annotate_pdf and true_pdf:\n",
    "            target_text = df1.text.to_list()\n",
    "            x = annotate_target_text(path=path, target_text=target_text)\n",
    "\n",
    "        # print completion status\n",
    "        app.write(\"Analysis completed! üëç\")\n",
    "        time_taken = round((time.time() - start_time)/60, 1)\n",
    "        l = [\"Time taken:\", str(time_taken), \"mins\"]\n",
    "        app.write(\" \".join(l))\n",
    "\n",
    "        # CSV file for user download\n",
    "        file_name = uploaded_file.name.replace('pdf', 'csv')\n",
    "        csv = df.to_csv(index=False).encode('utf-8')\n",
    "        app.download_button(label=\"Download CSV output\", \n",
    "                            data=csv, \n",
    "                            file_name=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18128341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports this notebook to a .py file with streamlit statements added\n",
    "nb_export(\"NetZero-Reduction_Hosted.ipynb\", lib_path=\"./\", name=\"NetZero-Reduction_Hosted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
