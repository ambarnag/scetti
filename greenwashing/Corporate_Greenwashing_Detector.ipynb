{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d3f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import streamlit as st\n",
    "from streamlit_jupyter import StreamlitPatcher\n",
    "StreamlitPatcher().jupyter()\n",
    "from nbdev.export import nb_export\n",
    "\n",
    "# Set page configuration\n",
    "st.set_page_config(layout=\"wide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29965ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# generic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "# pdf processing functions\n",
    "import fitz\n",
    "import pdf_utils\n",
    "\n",
    "# NLP models\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7193dd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "# pipeline for base climate detector (stage-1 classifier)\n",
    "name = \"climatebert/distilroberta-base-climate-detector\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, max_len=512)\n",
    "detector_pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "810a83ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "# pipeline for sentiment classifier\n",
    "name = \"climatebert/distilroberta-base-climate-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, max_len=512)\n",
    "sentiment_pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7684956c-f565-42fe-82ab-8795f3849318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# pipeline for commitment classifier\n",
    "name = \"climatebert/distilroberta-base-climate-commitment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, max_len=512)\n",
    "commitment_pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1818f9ad-8ad0-4751-859f-cd8612b00db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# pipeline for specificity classifier\n",
    "name = \"climatebert/distilroberta-base-climate-specificity\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, max_len=512)\n",
    "specificity_pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4097d5b-a849-428a-959a-6254ba270c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# pipeline for TCFD classifier\n",
    "name = \"climatebert/distilroberta-base-climate-tcfd\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, max_len=512)\n",
    "tcfd_pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3d36c35-5ada-40cd-b865-e37826bc60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# function to encapsulate all preprocessing steps (condensed version w/o OCR support)\n",
    "def pdf_to_sentences(path, max_char, min_words=5):\n",
    "    text = pdf_utils.true_pdf2text(path=path, stream=True)\n",
    "    text = text[:max_char] # spacy has a 1M char limit\n",
    "    sents = pdf_utils.text2sents(text=text, min_words=min_words)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df46101-f0af-47c2-b4ee-5173cce11530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# returns climate-related sentences from a stage-1 classifier\n",
    "def classify_s1(sents):\n",
    "    # run stage-1 classifier\n",
    "    s1_class = detector_pipe(sents, padding=True, truncation=True)\n",
    "        \n",
    "    # subset sents whose s1_class label is 'yes'\n",
    "    y_index = [i for i, item in enumerate(s1_class) if item['label'] == 'yes']\n",
    "    sents_s1 = [sents[i] for i in y_index]\n",
    "    msg = ' '.join([str(len(sents_s1)), 'sentences are climate-related.', str(len(sents)-len(sents_s1)), 'sentences discarded.'])\n",
    "    app.write(msg)\n",
    "    \n",
    "    return sents_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99d37e2a-6521-4a1b-96b0-27de60c5276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# runs a stage-2 classifier on sentences based on task\n",
    "def classify_s2(sents, task):\n",
    "    if task == \"sentiment\":\n",
    "        s2_class = sentiment_pipe(sents, padding=True, truncation=True)\n",
    "    elif task == \"commitment\":\n",
    "        s2_class = commitment_pipe(sents, padding=True, truncation=True)\n",
    "    elif task == \"specificity\":\n",
    "        s2_class = specificity_pipe(sents, padding=True, truncation=True)\n",
    "    else:\n",
    "        s2_class = tcfd_pipe(sents, padding=True, truncation=True)\n",
    "        \n",
    "    # create df with texts and labels\n",
    "    s2_labels = [x['label'] for x in s2_class]\n",
    "    df = pd.DataFrame({\"text\": sents, task: s2_labels})\n",
    "    \n",
    "    # generate summary counts and plot\n",
    "    counts = df.groupby(task).count()\n",
    "    app.subheader(task.title())\n",
    "    app.bar_chart(data=counts)\n",
    "    summ = pd.DataFrame(counts).reset_index()\n",
    "    \n",
    "    return df, summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd036000-f38c-495f-9837-dd3d4a6269ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# run stage-1 and multiple stage-2 classifiers > collate outputs\n",
    "def classify(path, max_char=1000000):\n",
    "    # run preprocessing steps\n",
    "    sents = pdf_to_sentences(path, max_char, min_words=10)\n",
    "    \n",
    "    # NLP processing time per sentence based on test runs\n",
    "    est_time = round(0.15*len(sents)/60) \n",
    "    msg = (\"‚è≥Running classification... Estimated time remaining:\", str(est_time), \"mins\")\n",
    "    placeholder = app.empty()\n",
    "    placeholder.write(\" \".join(msg))\n",
    "\n",
    "    # run stage-1 classifier\n",
    "    sents_s1 = classify_s1(sents)\n",
    "    \n",
    "    # run s2 classifiers on s1 output\n",
    "    df_sentiment, summ_sentiment = classify_s2(sents_s1, \"sentiment\")\n",
    "    df_commitment, summ_commitment = classify_s2(sents_s1, \"commitment\")\n",
    "    df_specificity, summ_specificity = classify_s2(sents_s1, \"specificity\")\n",
    "    df_tcfd, summ_tcfd = classify_s2(sents_s1, \"TCFD\")\n",
    "    \n",
    "    # merge s2 labels into single df\n",
    "    df_list = [df_sentiment, df_commitment, df_specificity, df_tcfd]\n",
    "    tmp = [df.set_index(['text']) for df in df_list]\n",
    "    df = pd.concat(tmp, axis=1).reset_index()\n",
    "    \n",
    "    # merge summary counts\n",
    "    summ_list = [summ_sentiment, summ_commitment, summ_specificity, summ_tcfd]\n",
    "    summ = pd.concat(summ_list, axis=1).fillna(\"\")\n",
    "    \n",
    "    placeholder.empty()\n",
    "    \n",
    "    return(df, summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c9843ae-f6a5-4c9b-97b0-1f717688465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# calculate sustainability score\n",
    "def sustainability_score(summ):\n",
    "    n = int(summ.sum(axis=0, numeric_only=True)) # total no. of texts\n",
    "\n",
    "    pct_nn = float((summ.iloc[1,1]+summ.iloc[2,1])/n) # Non-neutral\n",
    "    pct_o = float(summ.iloc[1,1]/(summ.iloc[1,1]+summ.iloc[2,1])) # Opportunity vs. Risk\n",
    "    pct_c = float(summ.iloc[1,3]/n) # Commitment = Y\n",
    "    pct_sp = float(summ.iloc[1,5]/n) # Specific vs. Non-specific\n",
    "    pct_m = float(summ.iloc[1,7]/n) # TCFD Metrics\n",
    "    pct_st = float(summ.iloc[3,7]/n) # TCFD Strategy\n",
    "\n",
    "    score = round(100*np.average([pct_nn*pct_o, pct_c, pct_sp, np.average([pct_m, pct_st])]), 2)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27885ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# generate content for About page\n",
    "def setup_about_page(file):\n",
    "    # demo data download\n",
    "    with open(file, \"rb\") as pdf_file:\n",
    "        demo_data = pdf_file.read()\n",
    "    \n",
    "    txt_demo = '''üëãWelcome to the Corporate Greenwashing Detector tool!\n",
    "    If you do not have a corporate sustainability report available, you can try out the app with a sample report.'''\n",
    "    ack.markdown(txt_demo)\n",
    "    ack.download_button(label=\"Download demo document\",\n",
    "                        data=demo_data,\n",
    "                        file_name=\"Meta-2023-Sustainability-Report.pdf\",\n",
    "                        mime='application/octet-stream')    \n",
    "    \n",
    "    # acknowledgements\n",
    "    ack.subheader(\":star2: About ClimateBERT :star2:\", divider=\"blue\")\n",
    "    ack_file = r'D:\\Work\\Climate NLP\\proposals\\Backdrop Build v5\\acknowledgements.txt'\n",
    "    with open(ack_file) as f:\n",
    "        ack_text = f.read()\n",
    "    ack.markdown(ack_text)\n",
    "    \n",
    "    # contact info\n",
    "    txt_fb = '''üìßTell us about your experience with the Corporate Greenwashing Detector tool! Please send your \n",
    "    feedback and suggestions to <a href='mailto:ambar@scetti.org'> ambar@scetti.org</a>'''\n",
    "    ack.markdown(txt_fb, unsafe_allow_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1120d7cd-e287-42f1-908e-2f719f1ad699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# function to keep count of app runs\n",
    "def update_run_count():\n",
    "    run_count_file = r'D:\\Work\\Climate NLP\\web apps\\run_count.txt'\n",
    "\n",
    "    # read current value of run_count from file\n",
    "    with open(run_count_file, \"r\") as file:\n",
    "        run_count = int(file.read())\n",
    "\n",
    "    # increment the count and write it back to the file\n",
    "    run_count += 1\n",
    "    with open(run_count_file, \"w\") as file:\n",
    "        file.write(str(run_count))\n",
    "\n",
    "    # st.write(f\"This Streamlit app has been run {run_count}¬†times.\")\n",
    "    return run_count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b910b260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Corporate Greenwashing Detector"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 19:29:35.317 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\ambar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "st.title(\"Corporate Greenwashing Detector\")\n",
    "\n",
    "# user input widgets (form inside sidebar)\n",
    "with st.sidebar:\n",
    "    with st.form(\"user_inputs\"):\n",
    "        uploaded_file = st.file_uploader(label=\"Upload a PDF document\",\n",
    "                                         type=[\".pdf\"])\n",
    "        submitted = st.form_submit_button(\"Analyze document\")\n",
    "\n",
    "# create separate tabs for analysis and acknowledgements\n",
    "app, ack = st.tabs([\"üìàAnalysis\", \"üì¢About\"])\n",
    "\n",
    "# setup About page\n",
    "x = setup_about_page(file = r'D:\\Work\\Climate NLP\\web apps\\Meta-2023-Sustainability-Report.pdf')\n",
    "\n",
    "# description for Analysis tab\n",
    "app.markdown('''\n",
    "This app uses AI-based Natural Language Processing to score companies on the ambition, credibility, and feasibility of climate change mitigation \n",
    "and adaptation commitments as articulated in their corporate sustainability report. A **Sustainability Score** for a company is calculated based on \n",
    "the following parameters: \\n\\n\n",
    "‚ù§Ô∏è :blue[Sentiment]: How many climate-related statements are classified as characterizing Risk, Opportunity, and Neutral?  \n",
    "üíç :blue[Commitment]: How many statements demonstrate commitment to climate mitigation or adaptation goals?  \n",
    "üéØ :blue[Specificity]: How many statements are specific versus non-specific in nature?  \n",
    "üìù :blue[TCFD]: How many statements are classified under the TCFD pillars of Governance, Metrics, Risk, and Strategy?  \n",
    "''')\n",
    "\n",
    "if uploaded_file and submitted:\n",
    "    start_time = time.time()\n",
    "    x = update_run_count()\n",
    "    \n",
    "    # run 2-stage classification on a PDF (set max_char=1000 for testing)\n",
    "    path = uploaded_file.read()\n",
    "    df, summ = classify(path=path)\n",
    "    \n",
    "    if df is not None:\n",
    "        ss = sustainability_score(summ)\n",
    "        msg = \" \".join(['Corporate Sustainability Score based on', uploaded_file.name, '=', str(ss)])\n",
    "        app.write(msg)\n",
    "        \n",
    "        # print completion status\n",
    "        app.write(\"Analysis completed! üëç\")\n",
    "        time_taken = round((time.time() - start_time)/60, 1)\n",
    "        msg = \" \".join(['Time taken:', str(time_taken), 'mins'])\n",
    "        app.write(msg)\n",
    "\n",
    "        # CSV file for user download\n",
    "        file_name = uploaded_file.name.replace('pdf', 'csv')\n",
    "        csv = df.to_csv(index=False).encode('utf-8')\n",
    "        app.download_button(label=\"Download CSV output\", \n",
    "                            data=csv, \n",
    "                            file_name=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18128341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports this notebook to a .py file with streamlit statements added\n",
    "nb_export(\"Corporate_Greenwashing_Detector.ipynb\", lib_path=\"./\", name=\"Corporate_Greenwashing_Detector\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
